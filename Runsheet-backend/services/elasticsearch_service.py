"""
Elasticsearch service for Runsheet Logistics Platform
Handles all Elasticsearch operations including index management and data operations

Validates:
- Requirement 3.5: Implement circuit breakers for Elasticsearch
- Requirement 2.4: Return specific error code indicating database unavailability
- Requirement 7.1: Implement index lifecycle management policies for data tiering
"""

import os
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from elasticsearch import Elasticsearch
from dotenv import load_dotenv
from config.settings import get_settings
from resilience.circuit_breaker import CircuitBreaker, CircuitBreakerConfig, CircuitOpenException
from errors.codes import ErrorCode
from errors.exceptions import AppException, elasticsearch_unavailable, circuit_open

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)


class ElasticsearchService:
    """
    Elasticsearch service with circuit breaker protection.
    
    All Elasticsearch operations are wrapped with a circuit breaker to prevent
    cascading failures when the database is unavailable.
    
    Validates:
    - Requirement 3.5: Implement circuit breakers for Elasticsearch
    - Requirement 2.4: Return specific error code indicating database unavailability
    """
    
    def __init__(self):
        self.client = None
        self.settings = get_settings()
        
        # Initialize circuit breaker for Elasticsearch operations
        # Default: 3 failures, 30 second recovery timeout
        self._circuit_breaker = CircuitBreaker(
            name="elasticsearch",
            config=CircuitBreakerConfig(
                failure_threshold=3,
            )
        )
        
        self.connect()
    
    def connect(self):
        """Initialize Elasticsearch connection"""
        try:
            api_key = self.settings.elastic_api_key.strip('"')
            endpoint = self.settings.elastic_endpoint.strip('"')
            
            if not api_key or not endpoint:
                raise ValueError("ELASTIC_API_KEY and ELASTIC_ENDPOINT must be set in configuration")
            
            self.client = Elasticsearch(
                endpoint,
                api_key=api_key,
                verify_certs=True,
                request_timeout=30
            )
            
            # Test connection
            if self.client.ping():
                logger.info("‚úÖ Connected to Elasticsearch successfully")
                # Set up ILM policies before creating indices
                self.setup_ilm_policies()
                self.setup_indices()
                # Apply ILM policies to existing indices
                self.apply_ilm_policies_to_indices()
                # Validate index schemas match expected mappings
                self.validate_index_schemas()
            else:
                raise ConnectionError("Failed to ping Elasticsearch")
                
        except Exception as e:
            logger.error(f"‚ùå Failed to connect to Elasticsearch: {e}")
            raise
    
    def _check_ilm_available(self) -> bool:
        """
        Check if ILM (Index Lifecycle Management) is available on this Elasticsearch cluster.
        
        ILM requires specific license tiers (Basic+ for some features, Platinum for others).
        This method detects availability to avoid errors on clusters without ILM support.
        
        Returns:
            True if ILM is available, False otherwise
        """
        try:
            # Try to list ILM policies - this will fail if ILM is not available
            self.client.ilm.get_lifecycle()
            return True
        except Exception as e:
            error_str = str(e).lower()
            # Check for common indicators that ILM is not available
            if "no handler found" in error_str or "unknown setting" in error_str or "ilm" in error_str:
                logger.info("‚ÑπÔ∏è ILM (Index Lifecycle Management) is not available on this Elasticsearch cluster. "
                          "This is normal for serverless or basic tier deployments. Skipping ILM configuration.")
                return False
            # For other errors, assume ILM might be available but there's a different issue
            logger.debug(f"ILM availability check encountered error: {e}")
            return False
    
    def setup_ilm_policies(self):
        """
        Set up Index Lifecycle Management (ILM) policies for data tiering.
        
        Creates ILM policies that move old data to warm/cold tiers after 30 days.
        Gracefully skips if ILM is not available on the cluster.
        
        Validates:
        - Requirement 7.1: Implement index lifecycle management policies that move 
          old data to warm/cold tiers after 30 days
        """
        # Check if ILM is available before attempting to create policies
        if not self._check_ilm_available():
            self._ilm_available = False
            return
        
        self._ilm_available = True
        
        # Define ILM policies for different data types
        ilm_policies = {
            "runsheet-standard-policy": self._get_standard_ilm_policy(),
            "runsheet-analytics-policy": self._get_analytics_ilm_policy(),
            "runsheet-logs-policy": self._get_logs_ilm_policy(),
        }
        
        for policy_name, policy_body in ilm_policies.items():
            try:
                # Check if policy already exists
                try:
                    existing_policy = self.client.ilm.get_lifecycle(name=policy_name)
                    logger.info(f"üìã ILM policy already exists: {policy_name}")
                    # Update the policy if it exists
                    self.client.ilm.put_lifecycle(name=policy_name, body=policy_body)
                    logger.info(f"‚úÖ Updated ILM policy: {policy_name}")
                except Exception:
                    # Policy doesn't exist, create it
                    self.client.ilm.put_lifecycle(name=policy_name, body=policy_body)
                    logger.info(f"‚úÖ Created ILM policy: {policy_name}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to create/update ILM policy {policy_name}: {e}")
                # Continue with other policies even if one fails
    
    def _get_standard_ilm_policy(self) -> Dict[str, Any]:
        """
        Get the standard ILM policy for operational data (trucks, orders, inventory, etc.).
        
        Policy phases:
        - Hot: Active data, optimized for indexing and search
        - Warm: Data older than 30 days, read-only, optimized for search
        - Cold: Data older than 90 days, minimal resources
        - Delete: Data older than 365 days (optional, can be disabled)
        
        Validates:
        - Requirement 7.1: Move old data to warm/cold tiers after 30 days
        
        Returns:
            Dict containing the ILM policy configuration
        """
        return {
            "policy": {
                "phases": {
                    "hot": {
                        "min_age": "0ms",
                        "actions": {
                            "rollover": {
                                "max_age": "30d",
                                "max_primary_shard_size": "50gb"
                            },
                            "set_priority": {
                                "priority": 100
                            }
                        }
                    },
                    "warm": {
                        "min_age": "30d",
                        "actions": {
                            "set_priority": {
                                "priority": 50
                            },
                            "shrink": {
                                "number_of_shards": 1
                            },
                            "forcemerge": {
                                "max_num_segments": 1
                            },
                            "readonly": {}
                        }
                    },
                    "cold": {
                        "min_age": "90d",
                        "actions": {
                            "set_priority": {
                                "priority": 0
                            },
                            "allocate": {
                                "number_of_replicas": 0
                            }
                        }
                    }
                }
            }
        }
    
    def _get_analytics_ilm_policy(self) -> Dict[str, Any]:
        """
        Get the ILM policy for analytics data.
        
        Analytics data has a longer retention period and different tiering strategy:
        - Hot: Active data for real-time analytics
        - Warm: Data older than 30 days, still queryable for historical analysis
        - Cold: Data older than 180 days, archived for compliance
        
        Validates:
        - Requirement 7.1: Move old data to warm/cold tiers after 30 days
        
        Returns:
            Dict containing the ILM policy configuration
        """
        return {
            "policy": {
                "phases": {
                    "hot": {
                        "min_age": "0ms",
                        "actions": {
                            "rollover": {
                                "max_age": "30d",
                                "max_primary_shard_size": "50gb"
                            },
                            "set_priority": {
                                "priority": 100
                            }
                        }
                    },
                    "warm": {
                        "min_age": "30d",
                        "actions": {
                            "set_priority": {
                                "priority": 50
                            },
                            "forcemerge": {
                                "max_num_segments": 1
                            },
                            "readonly": {}
                        }
                    },
                    "cold": {
                        "min_age": "180d",
                        "actions": {
                            "set_priority": {
                                "priority": 0
                            },
                            "allocate": {
                                "number_of_replicas": 0
                            }
                        }
                    }
                }
            }
        }
    
    def _get_logs_ilm_policy(self) -> Dict[str, Any]:
        """
        Get the ILM policy for log data.
        
        Log data has shorter retention and aggressive tiering:
        - Hot: Recent logs for active debugging
        - Warm: Logs older than 7 days
        - Cold: Logs older than 30 days
        - Delete: Logs older than 90 days
        
        Validates:
        - Requirement 7.1: Move old data to warm/cold tiers after 30 days
        
        Returns:
            Dict containing the ILM policy configuration
        """
        return {
            "policy": {
                "phases": {
                    "hot": {
                        "min_age": "0ms",
                        "actions": {
                            "rollover": {
                                "max_age": "7d",
                                "max_primary_shard_size": "30gb"
                            },
                            "set_priority": {
                                "priority": 100
                            }
                        }
                    },
                    "warm": {
                        "min_age": "7d",
                        "actions": {
                            "set_priority": {
                                "priority": 50
                            },
                            "shrink": {
                                "number_of_shards": 1
                            },
                            "forcemerge": {
                                "max_num_segments": 1
                            },
                            "readonly": {}
                        }
                    },
                    "cold": {
                        "min_age": "30d",
                        "actions": {
                            "set_priority": {
                                "priority": 0
                            },
                            "allocate": {
                                "number_of_replicas": 0
                            }
                        }
                    },
                    "delete": {
                        "min_age": "90d",
                        "actions": {
                            "delete": {}
                        }
                    }
                }
            }
        }
    
    def apply_ilm_policies_to_indices(self):
        """
        Apply ILM policies to existing indices.
        
        Maps indices to their appropriate ILM policies:
        - trucks, orders, inventory, support_tickets, locations -> standard policy
        - analytics_events -> analytics policy
        
        Skips if ILM is not available on the cluster.
        
        Validates:
        - Requirement 7.1: Implement index lifecycle management policies
        """
        # Skip if ILM is not available
        if not getattr(self, '_ilm_available', False):
            logger.debug("Skipping ILM policy application - ILM not available on this cluster")
            return
        
        # Define index to policy mapping
        index_policy_mapping = {
            "trucks": "runsheet-standard-policy",
            "orders": "runsheet-standard-policy",
            "inventory": "runsheet-standard-policy",
            "support_tickets": "runsheet-standard-policy",
            "locations": "runsheet-standard-policy",
            "analytics_events": "runsheet-analytics-policy",
        }
        
        for index_name, policy_name in index_policy_mapping.items():
            try:
                # Check if index exists
                if self.client.indices.exists(index=index_name):
                    # Apply ILM policy to the index
                    self.client.indices.put_settings(
                        index=index_name,
                        body={
                            "index": {
                                "lifecycle": {
                                    "name": policy_name
                                }
                            }
                        }
                    )
                    logger.info(f"‚úÖ Applied ILM policy '{policy_name}' to index '{index_name}'")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to apply ILM policy to {index_name}: {e}")
                # Continue with other indices even if one fails
    
    def get_ilm_policy_status(self, index_name: str) -> Optional[Dict[str, Any]]:
        """
        Get the ILM status for a specific index.
        
        Args:
            index_name: Name of the index to check
            
        Returns:
            Dict containing ILM status information, or None if not available
            
        Validates:
        - Requirement 7.1: Index lifecycle management policies
        """
        try:
            response = self.client.ilm.explain_lifecycle(index=index_name)
            if "indices" in response and index_name in response["indices"]:
                return response["indices"][index_name]
            return None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to get ILM status for {index_name}: {e}")
            return None
    
    def get_all_ilm_policies(self) -> Dict[str, Any]:
        """
        Get all ILM policies configured in the cluster.
        
        Returns:
            Dict containing all ILM policies
            
        Validates:
        - Requirement 7.1: Index lifecycle management policies
        """
        try:
            return self.client.ilm.get_lifecycle()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to get ILM policies: {e}")
            return {}
    
    def update_ilm_policy(self, policy_name: str, policy_body: Dict[str, Any]) -> bool:
        """
        Update an existing ILM policy.
        
        Args:
            policy_name: Name of the policy to update
            policy_body: New policy configuration
            
        Returns:
            True if update was successful, False otherwise
            
        Validates:
        - Requirement 7.1: Index lifecycle management policies
        """
        try:
            self.client.ilm.put_lifecycle(name=policy_name, body=policy_body)
            logger.info(f"‚úÖ Updated ILM policy: {policy_name}")
            return True
        except Exception as e:
            logger.error(f"‚ùå Failed to update ILM policy {policy_name}: {e}")
            return False
    
    def remove_ilm_policy_from_index(self, index_name: str) -> bool:
        """
        Remove ILM policy from an index.
        
        Args:
            index_name: Name of the index
            
        Returns:
            True if removal was successful, False otherwise
            
        Validates:
        - Requirement 7.1: Index lifecycle management policies
        """
        try:
            self.client.indices.put_settings(
                index=index_name,
                body={
                    "index": {
                        "lifecycle": {
                            "name": None
                        }
                    }
                }
            )
            logger.info(f"‚úÖ Removed ILM policy from index: {index_name}")
            return True
        except Exception as e:
            logger.error(f"‚ùå Failed to remove ILM policy from {index_name}: {e}")
            return False
    
    @property
    def circuit_breaker(self) -> CircuitBreaker:
        """Get the circuit breaker instance for external access."""
        return self._circuit_breaker
    
    def _handle_circuit_breaker_exception(self, exc: CircuitOpenException) -> None:
        """
        Handle a circuit breaker exception by raising an appropriate AppException.
        
        Validates:
        - Requirement 2.4: Return specific error code indicating database unavailability
        - Requirement 3.2: Return service unavailable response immediately when circuit is open
        
        Args:
            exc: The CircuitOpenException that was raised
            
        Raises:
            AppException: With CIRCUIT_OPEN error code
        """
        time_until_retry = None
        if exc.time_until_retry:
            time_until_retry = int(exc.time_until_retry.total_seconds())
        
        raise circuit_open(
            message=f"Elasticsearch service temporarily unavailable. Circuit breaker '{exc.circuit_name}' is open.",
            details={
                "circuit_name": exc.circuit_name,
                "time_until_retry_seconds": time_until_retry,
                "service": "elasticsearch"
            }
        )
    
    def _handle_elasticsearch_error(self, operation: str, error: Exception) -> None:
        """
        Handle an Elasticsearch error by raising an appropriate AppException.
        
        Validates:
        - Requirement 2.4: Return specific error code indicating database unavailability
        
        Args:
            operation: The operation that failed (e.g., "search", "index")
            error: The exception that was raised
            
        Raises:
            AppException: With ELASTICSEARCH_UNAVAILABLE error code
        """
        logger.error(f"Elasticsearch {operation} failed: {error}")
        raise elasticsearch_unavailable(
            message=f"Database operation failed: {operation}",
            details={
                "operation": operation,
                "error": str(error)
            }
        )
    
    def setup_indices(self):
        """Create indices with proper mappings if they don't exist"""
        indices = {
            "trucks": self._get_trucks_mapping(),
            "locations": self._get_locations_mapping(),
            "orders": self._get_orders_mapping(),
            "inventory": self._get_inventory_mapping(),
            "support_tickets": self._get_support_tickets_mapping(),
            "analytics_events": self._get_analytics_mapping()
        }
        
        for index_name, mapping in indices.items():
            try:
                if not self.client.indices.exists(index=index_name):
                    self.client.indices.create(
                        index=index_name,
                        body=mapping
                    )
                    logger.info(f"‚úÖ Created index: {index_name}")
                else:
                    logger.info(f"üìã Index already exists: {index_name}")
            except Exception as e:
                logger.error(f"‚ùå Failed to create index {index_name}: {e}")
    
    def validate_index_schemas(self) -> Dict[str, Any]:
        """
        Validate that index mappings match expected schemas and log warnings for mismatches.
        
        This method compares the actual Elasticsearch index mappings against the expected
        schemas defined in the mapping methods. Any mismatches are logged as warnings.
        
        Validates:
        - Requirement 7.3: WHEN the Backend_Service starts, THE Elasticsearch_Client SHALL 
          verify index mappings match expected schemas and log warnings for mismatches
        
        Returns:
            Dict containing validation results with structure:
            {
                "valid": bool,
                "indices": {
                    "index_name": {
                        "valid": bool,
                        "mismatches": [list of mismatch descriptions]
                    }
                }
            }
        """
        logger.info("üîç Validating index schemas...")
        
        # Get expected mappings for all indices
        expected_mappings = {
            "trucks": self._get_trucks_mapping(),
            "locations": self._get_locations_mapping(),
            "orders": self._get_orders_mapping(),
            "inventory": self._get_inventory_mapping(),
            "support_tickets": self._get_support_tickets_mapping(),
            "analytics_events": self._get_analytics_mapping()
        }
        
        validation_results = {
            "valid": True,
            "indices": {}
        }
        
        for index_name, expected_mapping in expected_mappings.items():
            index_result = self._validate_single_index_schema(index_name, expected_mapping)
            validation_results["indices"][index_name] = index_result
            
            if not index_result["valid"]:
                validation_results["valid"] = False
        
        # Log summary
        if validation_results["valid"]:
            logger.info("‚úÖ All index schemas validated successfully")
        else:
            invalid_indices = [
                name for name, result in validation_results["indices"].items() 
                if not result["valid"]
            ]
            logger.warning(f"‚ö†Ô∏è Schema validation completed with mismatches in indices: {invalid_indices}")
        
        return validation_results
    
    def _validate_single_index_schema(self, index_name: str, expected_mapping: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate a single index's mapping against expected schema.
        
        Args:
            index_name: Name of the index to validate
            expected_mapping: Expected mapping configuration
            
        Returns:
            Dict with validation result:
            {
                "valid": bool,
                "mismatches": [list of mismatch descriptions],
                "missing_fields": [list of missing field names],
                "type_mismatches": [list of type mismatch descriptions],
                "extra_fields": [list of unexpected field names]
            }
            
        Validates:
        - Requirement 7.3: Verify index mappings match expected schemas
        """
        result = {
            "valid": True,
            "mismatches": [],
            "missing_fields": [],
            "type_mismatches": [],
            "extra_fields": []
        }
        
        try:
            # Check if index exists
            if not self.client.indices.exists(index=index_name):
                result["valid"] = False
                result["mismatches"].append(f"Index '{index_name}' does not exist")
                logger.warning(f"‚ö†Ô∏è Schema validation: Index '{index_name}' does not exist")
                return result
            
            # Get actual mapping from Elasticsearch
            actual_mapping_response = self.client.indices.get_mapping(index=index_name)
            actual_mapping = actual_mapping_response.get(index_name, {}).get("mappings", {})
            
            # Get expected properties
            expected_properties = expected_mapping.get("mappings", {}).get("properties", {})
            actual_properties = actual_mapping.get("properties", {})
            
            # Compare properties
            self._compare_properties(
                expected_properties, 
                actual_properties, 
                result, 
                index_name,
                path=""
            )
            
            # Log warnings for any mismatches
            if result["missing_fields"]:
                logger.warning(
                    f"‚ö†Ô∏è Schema validation [{index_name}]: Missing fields: {result['missing_fields']}"
                )
            
            if result["type_mismatches"]:
                for mismatch in result["type_mismatches"]:
                    logger.warning(f"‚ö†Ô∏è Schema validation [{index_name}]: {mismatch}")
            
            if result["extra_fields"]:
                logger.info(
                    f"‚ÑπÔ∏è Schema validation [{index_name}]: Extra fields in actual mapping "
                    f"(may be auto-generated): {result['extra_fields']}"
                )
            
            if result["valid"]:
                logger.info(f"‚úÖ Schema validation [{index_name}]: Mapping matches expected schema")
            
        except Exception as e:
            result["valid"] = False
            result["mismatches"].append(f"Failed to validate index '{index_name}': {str(e)}")
            logger.error(f"‚ùå Schema validation [{index_name}]: Failed to validate - {e}")
        
        return result
    
    def _compare_properties(
        self, 
        expected: Dict[str, Any], 
        actual: Dict[str, Any], 
        result: Dict[str, Any],
        index_name: str,
        path: str = ""
    ) -> None:
        """
        Recursively compare expected and actual property mappings.
        
        Args:
            expected: Expected properties mapping
            actual: Actual properties mapping from Elasticsearch
            result: Result dict to update with mismatches
            index_name: Name of the index being validated
            path: Current path in the property hierarchy (for nested fields)
            
        Validates:
        - Requirement 7.3: Verify index mappings match expected schemas
        """
        # Check for missing fields in actual mapping
        for field_name, expected_config in expected.items():
            full_path = f"{path}.{field_name}" if path else field_name
            
            if field_name not in actual:
                result["valid"] = False
                result["missing_fields"].append(full_path)
                result["mismatches"].append(f"Missing field: {full_path}")
                continue
            
            actual_config = actual[field_name]
            
            # Compare field types
            expected_type = expected_config.get("type")
            actual_type = actual_config.get("type")
            
            # Handle nested properties (objects without explicit type)
            if "properties" in expected_config:
                # This is an object type with nested properties
                if "properties" not in actual_config:
                    result["valid"] = False
                    result["type_mismatches"].append(
                        f"Field '{full_path}': Expected object with properties, "
                        f"but actual has no nested properties"
                    )
                    result["mismatches"].append(
                        f"Type mismatch at '{full_path}': expected object, got {actual_type}"
                    )
                else:
                    # Recursively compare nested properties
                    self._compare_properties(
                        expected_config["properties"],
                        actual_config.get("properties", {}),
                        result,
                        index_name,
                        full_path
                    )
            elif expected_type:
                # Compare explicit types
                if actual_type and expected_type != actual_type:
                    # Some type variations are acceptable (e.g., semantic_text might be stored differently)
                    if not self._is_compatible_type(expected_type, actual_type):
                        result["valid"] = False
                        result["type_mismatches"].append(
                            f"Field '{full_path}': Expected type '{expected_type}', "
                            f"but actual type is '{actual_type}'"
                        )
                        result["mismatches"].append(
                            f"Type mismatch at '{full_path}': expected {expected_type}, got {actual_type}"
                        )
        
        # Check for extra fields in actual mapping (informational, not a validation failure)
        for field_name in actual:
            full_path = f"{path}.{field_name}" if path else field_name
            if field_name not in expected:
                result["extra_fields"].append(full_path)
    
    def _is_compatible_type(self, expected_type: str, actual_type: str) -> bool:
        """
        Check if two Elasticsearch field types are compatible.
        
        Some type variations are acceptable due to Elasticsearch's type inference
        or plugin-specific types.
        
        Args:
            expected_type: The expected field type
            actual_type: The actual field type from Elasticsearch
            
        Returns:
            True if types are compatible, False otherwise
            
        Validates:
        - Requirement 7.3: Verify index mappings match expected schemas
        """
        # Define compatible type pairs
        compatible_types = {
            # semantic_text may be stored as text with additional inference config
            ("semantic_text", "text"): True,
            ("text", "semantic_text"): True,
            # long and integer are often interchangeable
            ("integer", "long"): True,
            ("long", "integer"): True,
            # float and double are often interchangeable
            ("float", "double"): True,
            ("double", "float"): True,
        }
        
        return compatible_types.get((expected_type, actual_type), False)
    
    def get_index_mapping(self, index_name: str) -> Optional[Dict[str, Any]]:
        """
        Get the current mapping for a specific index.
        
        Args:
            index_name: Name of the index
            
        Returns:
            Dict containing the index mapping, or None if index doesn't exist
            
        Validates:
        - Requirement 7.3: Verify index mappings match expected schemas
        """
        try:
            if not self.client.indices.exists(index=index_name):
                return None
            
            response = self.client.indices.get_mapping(index=index_name)
            return response.get(index_name, {}).get("mappings", {})
        except Exception as e:
            logger.error(f"‚ùå Failed to get mapping for index '{index_name}': {e}")
            return None
    
    def get_schema_validation_summary(self) -> Dict[str, Any]:
        """
        Get a summary of schema validation status for all indices.
        
        Returns:
            Dict containing validation summary with counts and details
            
        Validates:
        - Requirement 7.3: Verify index mappings match expected schemas
        """
        validation_results = self.validate_index_schemas()
        
        total_indices = len(validation_results["indices"])
        valid_indices = sum(
            1 for result in validation_results["indices"].values() 
            if result["valid"]
        )
        invalid_indices = total_indices - valid_indices
        
        total_mismatches = sum(
            len(result["mismatches"]) 
            for result in validation_results["indices"].values()
        )
        
        return {
            "overall_valid": validation_results["valid"],
            "total_indices": total_indices,
            "valid_indices": valid_indices,
            "invalid_indices": invalid_indices,
            "total_mismatches": total_mismatches,
            "details": validation_results["indices"]
        }
    
    def _get_trucks_mapping(self):
        """Get mapping for trucks index"""
        return {
            "mappings": {
                "properties": {
                    "truck_id": {"type": "keyword"},
                    "plate_number": {"type": "keyword"},
                    "driver_id": {"type": "keyword"},
                    "driver_name": {"type": "text"},
                    "current_location": {
                        "properties": {
                            "id": {"type": "keyword"},
                            "name": {"type": "text"},
                            "type": {"type": "keyword"},
                            "coordinates": {"type": "geo_point"},
                            "address": {"type": "text"}
                        }
                    },
                    "destination": {
                        "properties": {
                            "id": {"type": "keyword"},
                            "name": {"type": "text"},
                            "type": {"type": "keyword"},
                            "coordinates": {"type": "geo_point"},
                            "address": {"type": "text"}
                        }
                    },
                    "route": {
                        "properties": {
                            "id": {"type": "keyword"},
                            "distance": {"type": "float"},
                            "estimated_duration": {"type": "integer"},
                            "actual_duration": {"type": "integer"}
                        }
                    },
                    "status": {"type": "keyword"},
                    "estimated_arrival": {"type": "date"},
                    "last_update": {"type": "date"},
                    "cargo": {
                        "properties": {
                            "type": {"type": "keyword"},
                            "weight": {"type": "float"},
                            "volume": {"type": "float"},
                            "description": {"type": "semantic_text"},
                            "priority": {"type": "keyword"}
                        }
                    },
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            }
        }
    
    def _get_locations_mapping(self):
        """Get mapping for locations index"""
        return {
            "mappings": {
                "properties": {
                    "location_id": {"type": "keyword"},
                    "name": {"type": "text"},
                    "type": {"type": "keyword"},
                    "coordinates": {"type": "geo_point"},
                    "address": {"type": "semantic_text"},
                    "region": {"type": "keyword"},
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            }
        }
    
    def _get_orders_mapping(self):
        """Get mapping for orders index"""
        return {
            "mappings": {
                "properties": {
                    "order_id": {"type": "keyword"},
                    "customer": {"type": "text"},
                    "customer_id": {"type": "keyword"},
                    "status": {"type": "keyword"},
                    "value": {"type": "float"},
                    "items": {"type": "semantic_text"},
                    "truck_id": {"type": "keyword"},
                    "region": {"type": "keyword"},
                    "priority": {"type": "keyword"},
                    "created_at": {"type": "date"},
                    "delivery_eta": {"type": "date"},
                    "delivered_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            }
        }
    
    def _get_inventory_mapping(self):
        """Get mapping for inventory index"""
        return {
            "mappings": {
                "properties": {
                    "item_id": {"type": "keyword"},
                    "name": {"type": "semantic_text"},
                    "category": {"type": "keyword"},
                    "quantity": {"type": "integer"},
                    "unit": {"type": "keyword"},
                    "location": {"type": "text"},
                    "status": {"type": "keyword"},
                    "last_updated": {"type": "date"},
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"}
                }
            }
        }
    
    def _get_support_tickets_mapping(self):
        """Get mapping for support tickets index"""
        return {
            "mappings": {
                "properties": {
                    "ticket_id": {"type": "keyword"},
                    "customer": {"type": "text"},
                    "customer_id": {"type": "keyword"},
                    "issue": {"type": "semantic_text"},
                    "description": {"type": "semantic_text"},
                    "priority": {"type": "keyword"},
                    "status": {"type": "keyword"},
                    "assigned_to": {"type": "keyword"},
                    "related_order": {"type": "keyword"},
                    "created_at": {"type": "date"},
                    "updated_at": {"type": "date"},
                    "resolved_at": {"type": "date"}
                }
            }
        }
    
    def _get_analytics_mapping(self):
        """Get mapping for analytics events index"""
        return {
            "mappings": {
                "properties": {
                    "event_id": {"type": "keyword"},
                    "event_type": {"type": "keyword"},
                    "timestamp": {"type": "date"},
                    "truck_id": {"type": "keyword"},
                    "order_id": {"type": "keyword"},
                    "region": {"type": "keyword"},
                    "route_name": {"type": "text"},
                    "route_id": {"type": "keyword"},
                    "delay_cause": {"type": "keyword"},
                    "metrics": {
                        "properties": {
                            # Performance metrics
                            "delivery_performance_pct": {"type": "float"},
                            "average_delay_minutes": {"type": "float"},
                            "fleet_utilization_pct": {"type": "float"},
                            "customer_satisfaction": {"type": "float"},
                            "on_time_percentage": {"type": "float"},
                            
                            # Delivery metrics
                            "delivery_time_minutes": {"type": "integer"},
                            "delay_minutes": {"type": "integer"},
                            "distance_km": {"type": "float"},
                            "fuel_consumed_liters": {"type": "float"},
                            "customer_rating": {"type": "float"},
                            
                            # Count metrics
                            "total_deliveries": {"type": "integer"},
                            "on_time_deliveries": {"type": "integer"},
                            "active_trucks": {"type": "integer"},
                            "completed_trips": {"type": "integer"},
                            "delay_incidents": {"type": "integer"},
                            "incident_count": {"type": "integer"},
                            
                            # Performance analysis
                            "performance_pct": {"type": "float"},
                            "avg_delivery_time": {"type": "float"},
                            "percentage": {"type": "float"},
                            "avg_delay_minutes": {"type": "float"},
                            
                            # Planning metrics
                            "planned_distance_km": {"type": "float"},
                            "estimated_duration_minutes": {"type": "integer"},
                            "expected_delay_duration": {"type": "integer"}
                        }
                    },
                    "created_at": {"type": "date"}
                }
            }
        }
    
    # CRUD Operations
    async def index_document(self, index: str, doc_id: str, document: Dict[Any, Any]):
        """
        Index a single document with circuit breaker protection.
        
        Validates:
        - Requirement 3.5: Implement circuit breakers for Elasticsearch
        - Requirement 2.4: Return specific error code indicating database unavailability
        """
        try:
            async def _do_index():
                document["updated_at"] = datetime.now().isoformat()
                if "created_at" not in document:
                    document["created_at"] = datetime.now().isoformat()
                
                response = self.client.index(
                    index=index,
                    id=doc_id,
                    body=document,
                    refresh=True
                )
                return response
            
            return await self._circuit_breaker.execute(_do_index)
        except CircuitOpenException as e:
            self._handle_circuit_breaker_exception(e)
        except Exception as e:
            self._handle_elasticsearch_error(f"index_document({index})", e)
    
    async def bulk_index_documents(self, index: str, documents: List[Dict[Any, Any]]) -> Dict[str, Any]:
        """
        Bulk index multiple documents with circuit breaker protection and partial failure handling.
        
        This method handles partial failures in bulk operations by:
        - Continuing to process successful documents even when some fail
        - Logging detailed information about failed documents
        - Returning a result indicating partial success with counts
        
        Validates:
        - Requirement 3.5: Implement circuit breakers for Elasticsearch
        - Requirement 2.4: Return specific error code indicating database unavailability
        - Requirement 7.6: WHEN bulk indexing operations fail partially, THE Elasticsearch_Client 
          SHALL log failed documents and continue processing successful ones
        
        Args:
            index: The name of the Elasticsearch index
            documents: List of documents to index
            
        Returns:
            Dict containing:
            - success: bool indicating if all documents were indexed successfully
            - total: total number of documents attempted
            - successful: count of successfully indexed documents
            - failed: count of failed documents
            - errors: list of error details for failed documents
        """
        try:
            async def _do_bulk_index():
                from elasticsearch.helpers import bulk, BulkIndexError
                
                actions = []
                doc_id_map = {}  # Map action index to document info for error reporting
                
                for idx, doc in enumerate(documents):
                    doc["updated_at"] = datetime.now().isoformat()
                    if "created_at" not in doc:
                        doc["created_at"] = datetime.now().isoformat()
                    
                    # Map index names to correct ID fields
                    id_field_map = {
                        "trucks": "truck_id",
                        "inventory": "item_id", 
                        "support_tickets": "ticket_id",
                        "orders": "order_id",
                        "locations": "location_id",
                        "analytics_events": "event_id"
                    }
                    
                    # Get the correct ID field for this index
                    id_field = id_field_map.get(index, f"{index[:-1]}_id")
                    doc_id = doc.get("id") or doc.get(id_field)
                    
                    if not doc_id:
                        logger.warning(f"No ID found for document in {index} index. Available fields: {list(doc.keys())}")
                    
                    action = {
                        "_index": index,
                        "_id": doc_id,
                        "_source": doc
                    }
                    actions.append(action)
                    doc_id_map[idx] = {"doc_id": doc_id, "index": index}
                
                # Initialize result structure
                result = {
                    "success": True,
                    "total": len(documents),
                    "successful": 0,
                    "failed": 0,
                    "errors": []
                }
                
                try:
                    # Use raise_on_error=False to handle partial failures
                    # This allows us to continue processing even when some documents fail
                    success_count, errors = bulk(
                        self.client, 
                        actions, 
                        refresh=True,
                        raise_on_error=False,
                        raise_on_exception=False
                    )
                    
                    result["successful"] = success_count
                    
                    # Process any errors that occurred
                    if errors:
                        result["success"] = False
                        result["failed"] = len(errors)
                        
                        for error in errors:
                            # Extract error details from the bulk response
                            error_info = self._extract_bulk_error_info(error)
                            result["errors"].append(error_info)
                            
                            # Log each failed document with details
                            # Validates Requirement 7.6: log failed documents
                            logger.error(
                                f"‚ùå Bulk indexing failed for document in '{index}': "
                                f"doc_id={error_info.get('doc_id', 'unknown')}, "
                                f"error_type={error_info.get('error_type', 'unknown')}, "
                                f"reason={error_info.get('reason', 'unknown')}"
                            )
                        
                        # Log summary of partial failure
                        logger.warning(
                            f"‚ö†Ô∏è Bulk indexing to '{index}' completed with partial failures: "
                            f"{result['successful']}/{result['total']} documents indexed successfully, "
                            f"{result['failed']} documents failed"
                        )
                    else:
                        logger.info(f"‚úÖ Bulk indexed {result['successful']} documents to {index}")
                    
                    return result
                    
                except BulkIndexError as e:
                    # Handle BulkIndexError which contains details about failed documents
                    # This exception is raised when raise_on_error=True (not our case, but handle defensively)
                    result["success"] = False
                    result["failed"] = len(e.errors)
                    result["successful"] = result["total"] - result["failed"]
                    
                    for error in e.errors:
                        error_info = self._extract_bulk_error_info(error)
                        result["errors"].append(error_info)
                        
                        # Log each failed document
                        logger.error(
                            f"‚ùå Bulk indexing failed for document in '{index}': "
                            f"doc_id={error_info.get('doc_id', 'unknown')}, "
                            f"error_type={error_info.get('error_type', 'unknown')}, "
                            f"reason={error_info.get('reason', 'unknown')}"
                        )
                    
                    logger.warning(
                        f"‚ö†Ô∏è Bulk indexing to '{index}' completed with partial failures: "
                        f"{result['successful']}/{result['total']} documents indexed successfully, "
                        f"{result['failed']} documents failed"
                    )
                    
                    return result
            
            return await self._circuit_breaker.execute(_do_bulk_index)
        except CircuitOpenException as e:
            self._handle_circuit_breaker_exception(e)
        except Exception as e:
            self._handle_elasticsearch_error(f"bulk_index_documents({index})", e)
    
    def _extract_bulk_error_info(self, error: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extract detailed error information from a bulk operation error.
        
        This method parses the error structure returned by Elasticsearch bulk operations
        and extracts relevant information for logging and reporting.
        
        Validates:
        - Requirement 7.6: Log failed documents with details
        
        Args:
            error: The error dict from Elasticsearch bulk response
            
        Returns:
            Dict containing:
            - doc_id: The document ID that failed
            - index: The target index
            - error_type: The type of error (e.g., 'mapper_parsing_exception')
            - reason: Human-readable error reason
            - caused_by: Additional cause information if available
        """
        error_info = {
            "doc_id": None,
            "index": None,
            "error_type": None,
            "reason": None,
            "caused_by": None
        }
        
        try:
            # The error structure can vary based on the operation type (index, create, update, delete)
            # Common structure: {'index': {'_index': '...', '_id': '...', 'error': {...}, 'status': 400}}
            for operation_type in ['index', 'create', 'update', 'delete']:
                if operation_type in error:
                    op_result = error[operation_type]
                    error_info["doc_id"] = op_result.get("_id")
                    error_info["index"] = op_result.get("_index")
                    
                    if "error" in op_result:
                        error_detail = op_result["error"]
                        error_info["error_type"] = error_detail.get("type")
                        error_info["reason"] = error_detail.get("reason")
                        
                        # Extract caused_by if present (nested error details)
                        if "caused_by" in error_detail:
                            caused_by = error_detail["caused_by"]
                            error_info["caused_by"] = {
                                "type": caused_by.get("type"),
                                "reason": caused_by.get("reason")
                            }
                    break
            
            # If we couldn't parse the standard structure, store the raw error
            if error_info["error_type"] is None and error_info["reason"] is None:
                error_info["reason"] = str(error)
                
        except Exception as parse_error:
            # If parsing fails, store what we can
            logger.warning(f"Failed to parse bulk error details: {parse_error}")
            error_info["reason"] = str(error)
        
        return error_info
    
    async def search_documents(self, index: str, query: Dict[Any, Any], size: int = 100):
        """
        Search documents in an index with circuit breaker protection.
        
        Validates:
        - Requirement 3.5: Implement circuit breakers for Elasticsearch
        - Requirement 2.4: Return specific error code indicating database unavailability
        """
        try:
            async def _do_search():
                # Add size to query body if not already present
                if "size" not in query:
                    query["size"] = size
                
                response = self.client.search(
                    index=index,
                    body=query
                )
                return response
            
            return await self._circuit_breaker.execute(_do_search)
        except CircuitOpenException as e:
            self._handle_circuit_breaker_exception(e)
        except Exception as e:
            self._handle_elasticsearch_error(f"search_documents({index})", e)
    
    async def get_document(self, index: str, doc_id: str):
        """
        Get a single document by ID with circuit breaker protection.
        
        Validates:
        - Requirement 3.5: Implement circuit breakers for Elasticsearch
        - Requirement 2.4: Return specific error code indicating database unavailability
        """
        try:
            async def _do_get():
                response = self.client.get(index=index, id=doc_id)
                return response["_source"]
            
            return await self._circuit_breaker.execute(_do_get)
        except CircuitOpenException as e:
            self._handle_circuit_breaker_exception(e)
        except Exception as e:
            self._handle_elasticsearch_error(f"get_document({index}, {doc_id})", e)
    
    async def get_all_documents(self, index: str, size: int = 1000):
        """
        Get all documents from an index with circuit breaker protection.
        
        Validates:
        - Requirement 3.5: Implement circuit breakers for Elasticsearch
        - Requirement 2.4: Return specific error code indicating database unavailability
        """
        try:
            query = {
                "query": {"match_all": {}},
                "sort": [{"created_at": {"order": "desc"}}]
            }
            response = await self.search_documents(index, query, size)
            return [hit["_source"] for hit in response["hits"]["hits"]]
        except AppException:
            # Re-raise AppExceptions (already handled by search_documents)
            raise
        except Exception as e:
            self._handle_elasticsearch_error(f"get_all_documents({index})", e)
    
    async def semantic_search(self, index: str, text: str, fields: List[str], size: int = 10):
        """
        Perform semantic search using semantic_text fields with circuit breaker protection.
        
        Validates:
        - Requirement 3.5: Implement circuit breakers for Elasticsearch
        - Requirement 2.4: Return specific error code indicating database unavailability
        """
        try:
            query = {
                "query": {
                    "multi_match": {
                        "query": text,
                        "fields": fields,
                        "type": "best_fields"
                    }
                }
            }
            response = await self.search_documents(index, query, size)
            return [hit["_source"] for hit in response["hits"]["hits"]]
        except AppException:
            # Re-raise AppExceptions (already handled by search_documents)
            raise
        except Exception as e:
            self._handle_elasticsearch_error(f"semantic_search({index})", e)
    
    # Analytics-specific methods
    async def get_time_series_data(self, event_type: str, metric_field: str, time_range: str = "7d"):
        """
        Get time-series data for analytics charts with circuit breaker protection.
        
        Validates:
        - Requirement 3.5: Implement circuit breakers for Elasticsearch
        - Requirement 2.4: Return specific error code indicating database unavailability
        """
        try:
            # Calculate date range
            from datetime import datetime, timedelta
            now = datetime.now()
            
            if time_range == "24h":
                start_time = now - timedelta(hours=24)
                interval = "1h"
            elif time_range == "7d":
                start_time = now - timedelta(days=7)
                interval = "1d"
            elif time_range == "30d":
                start_time = now - timedelta(days=30)
                interval = "1d"
            else:  # 90d
                start_time = now - timedelta(days=90)
                interval = "1d"
            
            query = {
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"event_type": event_type}},
                            {"range": {"timestamp": {"gte": start_time.isoformat()}}}
                        ]
                    }
                },
                "aggs": {
                    "time_series": {
                        "date_histogram": {
                            "field": "timestamp",
                            "fixed_interval": interval,
                            "min_doc_count": 0
                        },
                        "aggs": {
                            "avg_metric": {
                                "avg": {"field": f"metrics.{metric_field}"}
                            }
                        }
                    }
                },
                "size": 0
            }
            
            response = await self.search_documents("analytics_events", query)
            buckets = response["aggregations"]["time_series"]["buckets"]
            
            return [
                {
                    "timestamp": bucket["key_as_string"],
                    "value": round(bucket["avg_metric"]["value"] or 0, 2)
                }
                for bucket in buckets
            ]
        except AppException:
            # Re-raise AppExceptions (already handled by search_documents)
            raise
        except Exception as e:
            self._handle_elasticsearch_error("get_time_series_data", e)
    
    async def get_route_performance_data(self):
        """
        Get route performance aggregation with circuit breaker protection.
        
        Validates:
        - Requirement 3.5: Implement circuit breakers for Elasticsearch
        - Requirement 2.4: Return specific error code indicating database unavailability
        """
        try:
            query = {
                "query": {"term": {"event_type": "route_performance"}},
                "aggs": {
                    "routes": {
                        "terms": {"field": "route_name.keyword", "size": 10},
                        "aggs": {
                            "avg_performance": {
                                "avg": {"field": "metrics.performance_pct"}
                            }
                        }
                    }
                },
                "size": 0
            }
            
            response = await self.search_documents("analytics_events", query)
            buckets = response["aggregations"]["routes"]["buckets"]
            
            return [
                {
                    "name": bucket["key"],
                    "performance": round(bucket["avg_performance"]["value"] or 0, 1)
                }
                for bucket in buckets
            ]
        except AppException:
            # Re-raise AppExceptions (already handled by search_documents)
            raise
        except Exception as e:
            self._handle_elasticsearch_error("get_route_performance_data", e)
    
    async def get_delay_causes_data(self):
        """
        Get delay causes aggregation with circuit breaker protection.
        
        Validates:
        - Requirement 3.5: Implement circuit breakers for Elasticsearch
        - Requirement 2.4: Return specific error code indicating database unavailability
        """
        try:
            query = {
                "query": {"term": {"event_type": "delay_cause_analysis"}},
                "aggs": {
                    "causes": {
                        "terms": {"field": "delay_cause.keyword", "size": 10},
                        "aggs": {
                            "avg_percentage": {
                                "avg": {"field": "metrics.percentage"}
                            }
                        }
                    }
                },
                "size": 0
            }
            
            response = await self.search_documents("analytics_events", query)
            buckets = response["aggregations"]["causes"]["buckets"]
            
            return [
                {
                    "name": bucket["key"],
                    "percentage": round(bucket["avg_percentage"]["value"] or 0, 1)
                }
                for bucket in buckets
            ]
        except AppException:
            # Re-raise AppExceptions (already handled by search_documents)
            raise
        except Exception as e:
            self._handle_elasticsearch_error("get_delay_causes_data", e)
    
    async def get_regional_performance_data(self):
        """
        Get regional performance aggregation with circuit breaker protection.
        
        Validates:
        - Requirement 3.5: Implement circuit breakers for Elasticsearch
        - Requirement 2.4: Return specific error code indicating database unavailability
        """
        try:
            query = {
                "query": {"term": {"event_type": "regional_performance"}},
                "aggs": {
                    "regions": {
                        "terms": {"field": "region.keyword", "size": 10},
                        "aggs": {
                            "avg_on_time": {
                                "avg": {"field": "metrics.on_time_percentage"}
                            }
                        }
                    }
                },
                "size": 0
            }
            
            response = await self.search_documents("analytics_events", query)
            buckets = response["aggregations"]["regions"]["buckets"]
            
            return [
                {
                    "name": bucket["key"],
                    "onTimePercentage": round(bucket["avg_on_time"]["value"] or 0, 1)
                }
                for bucket in buckets
            ]
        except AppException:
            # Re-raise AppExceptions (already handled by search_documents)
            raise
        except Exception as e:
            self._handle_elasticsearch_error("get_regional_performance_data", e)
    
    async def get_current_metrics(self):
        """
        Get current performance metrics with circuit breaker protection.
        
        Validates:
        - Requirement 3.5: Implement circuit breakers for Elasticsearch
        - Requirement 2.4: Return specific error code indicating database unavailability
        """
        try:
            query = {
                "query": {"term": {"event_type": "daily_performance"}},
                "sort": [{"timestamp": {"order": "desc"}}],
                "size": 1
            }
            
            response = await self.search_documents("analytics_events", query)
            if response["hits"]["hits"]:
                latest = response["hits"]["hits"][0]["_source"]["metrics"]
                return {
                    "delivery_performance": {
                        "title": "Delivery Performance",
                        "value": f"{latest.get('delivery_performance_pct', 87.5)}%",
                        "change": "+2.3%",
                        "trend": "up"
                    },
                    "average_delay": {
                        "title": "Average Delay", 
                        "value": f"{latest.get('average_delay_minutes', 144)/60:.1f} hrs",
                        "change": "-0.8 hrs",
                        "trend": "down"
                    },
                    "fleet_utilization": {
                        "title": "Fleet Utilization",
                        "value": f"{latest.get('fleet_utilization_pct', 92)}%",
                        "change": "+5%",
                        "trend": "up"
                    },
                    "customer_satisfaction": {
                        "title": "Customer Satisfaction",
                        "value": f"{latest.get('customer_satisfaction', 4.2)}/5",
                        "change": "+0.1",
                        "trend": "up"
                    }
                }
            else:
                raise Exception("No analytics data found")
        except AppException:
            # Re-raise AppExceptions (already handled by search_documents)
            raise
        except Exception as e:
            self._handle_elasticsearch_error("get_current_metrics", e)

# Global instance
elasticsearch_service = ElasticsearchService()